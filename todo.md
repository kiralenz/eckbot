* ~~github~~
* check chat capability of local llm via ollama